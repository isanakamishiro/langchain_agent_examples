{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "238751a3-bc96-4b0a-9821-ec722d92e56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LangChain 1.0 Custom Middleware\n",
    "\n",
    "Ref:\n",
    "\n",
    "- https://docs.langchain.com/oss/python/langchain/middleware#custom-middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "357ccaea-86f9-47cf-affe-99fc4c1ecdc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain>=1.0.0 langchain_openai>=1.0.0 mlflow tenacity\n",
    "\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac46ba61-2705-4f4d-83d1-afded2a7fe17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## モデルとの接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1766dac6-7da0-4134-b53d-f09a395d0478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "creds = mlflow.utils.databricks_utils.get_databricks_host_creds()\n",
    "model = init_chat_model(\n",
    "    # \"openai:databricks-gpt-oss-20b\",\n",
    "    \"openai:databricks-qwen3-next-80b-a3b-instruct\",\n",
    "    api_key=creds.token,\n",
    "    base_url=creds.host + \"/serving-endpoints\",\n",
    ")\n",
    "\n",
    "# model.invoke(\"Hello\")\n",
    "\n",
    "# 以下のように環境変数設定でもできる\n",
    "# os.environ[\"OPENAI_API_KEY\"] = creds.token\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = creds.host + \"/serving-endpoints\"\n",
    "\n",
    "# model = init_chat_model(\"openai:databricks-gpt-oss-20b\")\n",
    "# model.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb14842-b97a-4e00-bbe8-40aa02fe894f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"指定した都市の天気を取得します。\"\"\"\n",
    "    return f\"It's always sunny in {city}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d8f477-fe95-4ce6-8be8-850ebc69f5a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Decorator-based middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531355a6-b797-4631-aac2-411b80a7bcfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import (\n",
    "    before_model,\n",
    "    after_model,\n",
    "    wrap_model_call,\n",
    "    before_agent,\n",
    "    after_agent,\n",
    "    wrap_tool_call,\n",
    "    AgentState,\n",
    "    ModelRequest,\n",
    "    ModelResponse,\n",
    "    dynamic_prompt,\n",
    ")\n",
    "from langchain.messages import AIMessage\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "# エージェント実行前のロギング\n",
    "@before_agent\n",
    "def log_before_agent(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    print(\"Agent処理を開始します\")\n",
    "    print(\n",
    "        f\"before_agent: {len(state['messages'])}件のメッセージでエージェントを呼び出そうとしています\"\n",
    "    )\n",
    "\n",
    "    return None\n",
    "\n",
    "# ノードスタイル: エージェント実行後のロギング\n",
    "@after_agent\n",
    "def log_after_agent(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    print(\"Agent処理を終了します\")\n",
    "    print(\n",
    "        f\"after_agent: {len(state['messages'])}件のメッセージでエージェント処理を完了しました\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ノードスタイル: モデル呼び出し前のロギング\n",
    "@before_model\n",
    "def log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    print(\n",
    "        f\"before_model: {len(state['messages'])}件のメッセージでモデルを呼び出そうとしています\"\n",
    "    )\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ノードスタイル: モデル呼び出し後のバリデーション\n",
    "@after_model(can_jump_to=[\"end\"])\n",
    "def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"BLOCKED\" in last_message.content:\n",
    "        return {\n",
    "            \"messages\": [AIMessage(\"そのリクエストには対応できません。\")],\n",
    "            \"jump_to\": \"end\",\n",
    "        }\n",
    "    return None\n",
    "\n",
    "\n",
    "# ラップスタイル: リトライロジック\n",
    "@wrap_model_call\n",
    "def retry_model(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelResponse:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            print(f\"エラー発生後にリトライ {attempt + 1}/3: {e}\")\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def retry_tool(\n",
    "    request: ModelRequest,\n",
    "    handler: Callable[[ModelRequest], ModelResponse],\n",
    ") -> ModelResponse:\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                raise\n",
    "            print(f\"エラー発生後にリトライ {attempt + 1}/3: {e}\")\n",
    "\n",
    "\n",
    "# カスタムミドルウェアを指定したエージェントを作成\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    middleware=[\n",
    "        log_before_agent,\n",
    "        log_after_agent,\n",
    "        log_before_model,\n",
    "        validate_output,\n",
    "        retry_model,\n",
    "        retry_tool,\n",
    "    ],\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "# ストリームで実行出力\n",
    "for stream_mode, chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"東京の天気を教えて\"}]},\n",
    "    stream_mode=[\"updates\"],\n",
    "):\n",
    "    print(f\"stream_mode: {stream_mode}\")\n",
    "    print(f\"content: {chunk}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e427a0-dd25-425d-b404-d824b59c846b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Class-based middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d4c99a-5247-45b3-a4b0-3c758742a939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware, AgentState\n",
    "from langgraph.runtime import Runtime\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LoggingMiddleware(AgentMiddleware):\n",
    "    def before_model(\n",
    "        self, state: AgentState, runtime: Runtime\n",
    "    ) -> dict[str, Any] | None:\n",
    "        print(f\"About to call model with {len(state['messages'])} messages\")\n",
    "        return None\n",
    "\n",
    "    def after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
    "        print(f\"Model returned: {state['messages'][-1].content}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# クラスベースのミドルウェアをエージェントで使用\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    middleware=[\n",
    "        LoggingMiddleware(),\n",
    "    ],\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "for stream_mode, chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"東京の天気を教えて\"}]},\n",
    "    stream_mode=[\"updates\"],\n",
    "):\n",
    "    print(f\"stream_mode: {stream_mode}\")\n",
    "    print(f\"content: {chunk}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81da0c3-2caf-4aa2-b1f4-edbd6a368cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\n",
    "from typing import Callable\n",
    "from mlflow.entities import SpanType\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed, wait_exponential, RetryError, after_log\n",
    "import time\n",
    "from collections.abc import Awaitable\n",
    "\n",
    "class RetryMiddleware(AgentMiddleware):\n",
    "    def __init__(self, max_retries: int = 3):\n",
    "        super().__init__()\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "    def wrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse],\n",
    "    ) -> ModelResponse:\n",
    "\n",
    "        @retry(\n",
    "            stop=stop_after_attempt(3),\n",
    "            wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "        )\n",
    "        def wrap_handler(request):\n",
    "            with mlflow.start_span(name=\"model_call\", span_type=SpanType.CHAIN) as span:\n",
    "                span.set_inputs(request)\n",
    "                return handler(request)\n",
    "\n",
    "        return wrap_handler(request)\n",
    "\n",
    "    async def awrap_model_call(\n",
    "        self,\n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], Awaitable[ModelResponse]],\n",
    "    ) -> ModelResponse:\n",
    "\n",
    "        @retry(\n",
    "            stop=stop_after_attempt(3),\n",
    "            wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "        )\n",
    "        async def awrap_handler(request):\n",
    "            with mlflow.start_span(name=\"model_call\", span_type=SpanType.CHAIN) as span:\n",
    "                span.set_inputs(request)\n",
    "                return await handler(request)\n",
    "\n",
    "        return await awrap_handler(request)\n",
    "\n",
    "\n",
    "# クラスベースのミドルウェアをエージェントで使用\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    middleware=[\n",
    "        RetryMiddleware(),\n",
    "    ],\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "for stream_mode, chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"東京の天気を教えて\"}]},\n",
    "    stream_mode=[\"updates\"],\n",
    "):\n",
    "    print(f\"stream_mode: {stream_mode}\")\n",
    "    print(f\"content: {chunk}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca849fee-459e-4f99-903c-6c829e5440c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agent\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "012_custom_middleware",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
